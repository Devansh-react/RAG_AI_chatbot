from schema.Models import State
from langchain_core.messages import HumanMessage, AIMessage
from utilities.LLM_init import llm

def final_answer(state: State):
    last_humman_message = next((m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)
    history_messages = state["messages"]
    if not last_humman_message:
        raise ValueError("No HumanMessage found in state.")
    
    system_prompt = """
    You're the final responder in a smart AI assistant.

    The user's question couldn’t be answered using internal tools or web results — so now, you're stepping in with your own best understanding.

    Your job is to:
    1. Provide a thoughtful, accurate, and clear answer based on your general world knowledge.
    2. Let the user know this answer is generated by the AI itself, not pulled from documents or search engines.
    3. Keep your tone friendly, respectful, and helpful — imagine you're chatting with a curious person, not just completing a task.

    Guidelines:
    - Avoid technical jargon (no mentions of RAG, embeddings, etc.).
    - If the question is vague or unclear, make reasonable assumptions and explain them briefly.
    - Feel free to add a short follow-up suggestion to keep the conversation going if it feels natural.
    - Keep the response structured and easy to read — bullets or short paragraphs work well.

    Remember, your goal is to be useful and human-like. Make the user feel heard and helped.
    """
    chat_history = []
    for msg in history_messages:
        if isinstance(msg, HumanMessage):
            chat_history.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            chat_history.append({"role": "assistant", "content": msg.content}) 

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": last_humman_message.content}
    ]+chat_history


    reply = llm.invoke(messages)

    state["messages"].append(AIMessage(content=reply.content))
    
    return state
