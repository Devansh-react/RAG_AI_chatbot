from schema.Models import State
from langchain_core.messages import HumanMessage, AIMessage
from utilities.LLM_init import llm

def final_answer(state: State):
    last_humman_message = next((m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)
    if not last_humman_message:
        raise ValueError("No HumanMessage found in state.")
    
    system_prompt = """
    You are the final answering agent in a multi-stage AI assistant.

    The user asked a question, but:
    - No sufficient answer could be found in the internal knowledge base (RAG).
    - A web search was not required or not used.

    Your job is to:
    1. Provide a complete, helpful, and clear answer using your own general world knowledge.
    2. Mention that this answer is being generated by the AI itself and not based on external documents or search results.
    3. Structure the response in a helpful way for the user.

    Guidelines:
    - Be accurate and concise.
    - If the question is ambiguous, assume a reasonable interpretation and clearly state any assumptions you make.
    - DO NOT refer to RAG, embeddings, or technical details.

    Example ending:  
    "This answer was generated directly by the AI model based on general knowledge."

    Respond helpfully, respectfully, and clearly.
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": last_humman_message.content}
    ]

    reply = llm.invoke(messages)

    state["messages"].append(AIMessage(content=reply.content))
    
    return state
